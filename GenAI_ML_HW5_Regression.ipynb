{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kagiany/2025/blob/main/GenAI_ML_HW5_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2025 GENAI-ML-HW5\n",
        "# Regression\n",
        "In this assignment, you are expected to use linear regression and multi layer perceptron (MLP) model to predict the metacritic score on the famous game platform, Steam. For more information, please check the homework slide.\n",
        "HW5 Slide Link :\n",
        "\n",
        "https://docs.google.com/presentation/d/1ysys__L1HKLPV2LX0u-KMP0LD1XamhNCY_hq29k-I0A/edit?usp=sharing\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLXCb4JlIRcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU Status"
      ],
      "metadata": {
        "id": "4wVpyn6GMqDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXS4iVpeMunb",
        "outputId": "4f4e14a5-87e1-4d62-ec90-17c090052966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 22 04:15:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Modules\n"
      ],
      "metadata": {
        "id": "haW0xxPKId40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FQrrCfFH7HY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Random Seed"
      ],
      "metadata": {
        "id": "D2PL6UqEJktd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 設定亂數種子，可以固定結果 =====\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# TODO - 自行選擇亂數種子\n",
        "set_seed(20251024)"
      ],
      "metadata": {
        "id": "O3h1fo0pIjwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "Eia1RChJJqM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/murphy-cthsu/GENAI-ML-2025-HW5-Data.git\n",
        "!mv GENAI-ML-2025-HW5-Data/*.csv .\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9p6dlZHKKBp",
        "outputId": "337e513f-ae33-4705-be0c-b19cc4e17411"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GENAI-ML-2025-HW5-Data'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 10 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 3.75 MiB | 10.34 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview Training Data"
      ],
      "metadata": {
        "id": "UcO_JK6fH2Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 載入訓練資料集 =====\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "# 預覽資料集\n",
        "row_index = 0\n",
        "row = train_df.iloc[row_index]\n",
        "for col, val in row.items():\n",
        "    if isinstance(val, str) and len(val) > 100:\n",
        "        print(f\"{col:25}: {val[:100]}...\")   # truncate long text\n",
        "    else:\n",
        "        print(f\"{col:25}: {val}\")\n",
        "# 數值與文字特徵\n",
        "numeric_features = [c for c in train_df.select_dtypes(include=['number']).columns if c != 'metacritic_score']\n",
        "print(\"All available numeric features :\", numeric_features)\n",
        "print(\"All available text features :\", train_df.select_dtypes(include=['object']).columns.tolist())"
      ],
      "metadata": {
        "id": "PFNzo0-MH1FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "c2GsyL8CkMQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TODO(Boss baseline) - Feature Selection: 選擇適合的features進行訓練，可選features請參見csv檔欄位\n",
        "# To check all available numeric features, uncomment the line below :\n",
        "# print(\"All availbale numeric features :\", train_df.select_dtypes(include=['number']).columns.tolist())\n",
        "numeric_features = ['recommendations', 'positive', 'negative','price']\n",
        "\n",
        "X_numeric = train_df[numeric_features].fillna(0).values\n",
        "y = train_df['metacritic_score'].fillna(0).values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# 預設只使用數值特徵\n",
        "X = X_numeric\n",
        "\n",
        "# TODO(Boss baseline) - Feature Selection: 使用文字欄位的Embedding捕捉特徵\n",
        "\"\"\"\n",
        "# 如果想要使用embedding，可以取消以下程式碼註解\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 要進行embedding的文字欄位\n",
        "text_columns = ['reviews','short_description']\n",
        "\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def extract_embeddings(df, col):\n",
        "    print(f\"Embedding column: {col}\")\n",
        "    texts = df[col].fillna(\"\").astype(str).tolist()\n",
        "    emb = embedder.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "    return emb\n",
        "\n",
        "def reduce_dim_with_pca(embeddings, n_components=64):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced = pca.fit_transform(embeddings)\n",
        "    return reduced, pca\n",
        "\n",
        "# 建立embeddings並用PCA降維\n",
        "embeddings_reduced = []\n",
        "pca_models = {}\n",
        "for col in text_columns:\n",
        "    emb = extract_embeddings(train_df, col)\n",
        "    emb_reduced, pca_model = reduce_dim_with_pca(emb, n_components=100)\n",
        "    embeddings_reduced.append(emb_reduced)\n",
        "    pca_models[col] = pca_model\n",
        "\n",
        "X_text_reduced = np.concatenate(embeddings_reduced, axis=1)\n",
        "\n",
        "# 將數值特徵與文字特徵結合\n",
        "X = np.concatenate([X_numeric, X_text_reduced], axis=1)\n",
        "\"\"\"\n",
        "\n",
        "# ===== 對資料進行標準化 =====\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "target_scaler = StandardScaler()\n",
        "y_scaled = target_scaler.fit_transform(y)\n",
        "\n",
        "# ===== 切分 train/valid set: 可以自行調整訓練集與驗證集的比例 =====\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_scaled, y_scaled, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "uQs0MNJ-kLhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Class\n"
      ],
      "metadata": {
        "id": "l6jHQgLVjoFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 資料集 =====\n",
        "class SteamDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "6vv4dyW4jq-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Class"
      ],
      "metadata": {
        "id": "rI5t6Fi4jz8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(Strong baseline) - 模型架構: 自行調整模型架構以提高模型能力\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=(128, 64), dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]), # 第一層：input_dim -> 128，後接ReLU，好處是能夠引入非線性，讓模型學習更複雜的模式\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout), # 設置Dropout能夠防止overfitting\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]), # 中間層：128 -> 64，接ReLU\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dims[1], 1), # 輸出層：64 -> 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "qXBN7r8WjzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter\n"
      ],
      "metadata": {
        "id": "C4Fe98JCC6-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(Medium baseline) - Training Hyperparameters: 自行調整訓練超參數來改善模型表現\n",
        "n_epochs = 1000          # 訓練回合數\n",
        "learning_rate = 1e-5     # 學習率\n",
        "batch_size = 128         # 每次訓練取多少樣本\n",
        "weight_decay = 0         # L2 regularization強度，緩解overfitting。Useful Reference:https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9\n",
        "shuffle_data = True      # 是否在每個 epoch 打亂資料訓練\n"
      ],
      "metadata": {
        "id": "GCKUAW0xC-1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "i9pZ_S37KjBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== 用DataLoader讀取資料集 =====\n",
        "train_set = SteamDataset(X_train, y_train)\n",
        "dev_set = SteamDataset(X_dev, y_dev)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle_data)\n",
        "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ===== Model / Optimizer / Loss =====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LinearModel(input_dim=X_train.shape[1]).to(device)  # TODO(Strong baseline) - 也可以換成 MLPModel\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "loss_fn = nn.MSELoss()\n",
        "# ===== 訓練迴圈 =====\n",
        "train_loss_history = []\n",
        "val_rmse_history = []\n",
        "for epoch in tqdm(range(n_epochs), desc=\"Training Progress\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        preds = model(x_batch)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_loss)\n",
        "    # 驗證\n",
        "    model.eval()\n",
        "    preds_list, y_true_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in dev_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            preds = model(x_batch)\n",
        "            preds_list.append(preds.cpu().numpy())  # move back to CPU for numpy\n",
        "            y_true_list.append(y_batch.cpu().numpy())\n",
        "    y_pred_raw = np.vstack(preds_list)\n",
        "    y_true_raw = np.vstack(y_true_list)\n",
        "\n",
        "    y_pred_inv = target_scaler.inverse_transform(y_pred_raw)\n",
        "    y_true_inv = target_scaler.inverse_transform(y_true_raw)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
        "    val_rmse_history.append(rmse)\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Val RMSE={rmse:.4f}\")\n",
        "# ===== Plot Curves =====\n",
        "epochs = np.arange(1, n_epochs + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, train_loss_history, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, val_rmse_history, label=\"Validation RMSE\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE (on original scale)\")\n",
        "plt.title(\"Validation RMSE Curve\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L-nbR_1HK-LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on Test Dataset"
      ],
      "metadata": {
        "id": "FjqjWH8Ekd7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 載入測試集 =====\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# 處理測試集 numeric 特徵\n",
        "X_test_num = test_df[numeric_features].fillna(0).values\n",
        "X_test_combined = np.hstack([X_test_num])\n",
        "\n",
        "# TODO: If sentence embedding used.\n",
        "\"\"\"\n",
        "# 如果有使用embedding，請取消以下程式碼註解\n",
        "# 處理文字embedding\n",
        "reviews_embed = embedder.encode(test_df[\"reviews\"].fillna(\"\").astype(str).tolist(),\n",
        "                               batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "short_desc_embed = embedder.encode(test_df[\"short_description\"].fillna(\"\").astype(str).tolist(),\n",
        "                                   batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# 用train時的PCA來降維\n",
        "reviews_embed_reduced = pca_models['reviews'].transform(reviews_embed)\n",
        "short_desc_embed_reduced = pca_models['short_description'].transform(short_desc_embed)\n",
        "\n",
        "# 合併numeric+embedding\n",
        "X_test_combined = np.hstack([X_test_num, reviews_embed_reduced, short_desc_embed_reduced])\n",
        "\"\"\"\n",
        "# 用 train 的 scaler transform (跟 train 特徵維度一致)\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "# ===== 模型推論 =====\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds_scaled = model(X_test_tensor).cpu().numpy()\n",
        "    preds = target_scaler.inverse_transform(preds_scaled).squeeze()\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"appid\": test_df[\"appid\"],\n",
        "    \"metacritic_score\": np.round(preds).astype(int)\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Output saved to submission.csv\")"
      ],
      "metadata": {
        "id": "9cc4McIISK44"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}