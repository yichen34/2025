from django.core.management.base import BaseCommand
from django.conf import settings
from django.utils import timezone

from pathlib import Path
import requests, time, json, tempfile, os, re
from bs4 import BeautifulSoup

URL = "https://ag.shuhao.idv.tw/content/ag/RiceData.aspx"
HEADERS = {"User-Agent": "Mozilla/5.0 (DjangoBot/1.0)"}
# æ”¯æ´ 2025/11/7 æˆ– 2025-11-07 å…©ç¨®æ ¼å¼
DATE_RE = re.compile(r"^\d{4}[-/]\d{1,2}[-/]\d{1,2}$")


# å…¨éƒ¨ç¸£å¸‚çš„ã€Œå¸‚å ´ã€æ¸…å–®ï¼ˆé é¢æŸ¥è©¢ä¸‹æ‹‰é¸å–®å‡ºç¾è€…ï¼‰
MARKETS = [
    "åŸºéš†å¸‚","å°åŒ—å¸‚","æ–°åŒ—å¸‚","æ¡ƒåœ’å¸‚","æ–°ç«¹å¸‚","æ–°ç«¹ç¸£","è‹—æ —ç¸£",
    "å°ä¸­å¸‚","å½°åŒ–ç¸£","å—æŠ•ç¸£","é›²æ—ç¸£","å˜‰ç¾©å¸‚","å˜‰ç¾©ç¸£","å°å—å¸‚",
    "é«˜é›„å¸‚","å±æ±ç¸£","å®œè˜­ç¸£","èŠ±è“®ç¸£","å°æ±ç¸£","æ¾æ¹–ç¸£"
]

COLUMNS = [
    "äº¤æ˜“æ—¥æœŸ", "å¸‚å ´",
    "ç²³ç¨®ç™½ç±³_é›¶å”®", "ç¡¬ç§ˆç™½ç±³_é›¶å”®", "è»Ÿç§ˆç™½ç±³_é›¶å”®", "åœ“ç³¯ç™½ç±³_é›¶å”®", "é•·ç³¯ç™½ç±³_é›¶å”®",
    "ç²³ç¨®ç™½ç±³_èº‰å”®", "ç¡¬ç§ˆç™½ç±³_èº‰å”®", "è»Ÿç§ˆç™½ç±³_èº‰å”®", "åœ“ç³¯ç™½ç±³_èº‰å”®", "é•·ç³¯ç™½ç±³_èº‰å”®",
]

def to_float(s: str) -> float:
    try:
        s = (s or "").strip().replace(",", "")
        return float(s) if s and s != "-" else 0.0
    except Exception:
        return 0.0

class Command(BaseCommand):
    help = "æŠ“å–æŒ‡å®šé æ•¸çš„ç±³åƒ¹è³‡æ–™ï¼Œä¾äº¤æ˜“æ—¥æœŸåˆ†çµ„ï¼ˆæ¯æ—¥è¡¨æ ¼é½Šå…¨ã€å»é‡ï¼‰ä¸¦å­˜æˆ JSON"

    def add_arguments(self, parser):
        parser.add_argument("--pages", type=int, default=1, help="è¦æŠ“å¹¾é è³‡æ–™ï¼ˆé è¨­ 1 é ï¼‰")

    def handle(self, *args, **opts):
        total_pages = opts["pages"]
        self.stdout.write(self.style.HTTP_INFO(f"é–‹å§‹æŠ“å–å‰ {total_pages} é çš„ç±³åƒ¹è³‡æ–™..."))

        all_rows = []
        seen = set()  # ç”¨æ–¼å»é‡ï¼š(date, market)

        for p in range(1, total_pages + 1):
            self.stdout.write(f"ğŸ“„ æŠ“å–ç¬¬ {p}/{total_pages} é ")

            html = self._fetch_page_html(p)
            if not html:
                self.stderr.write(self.style.ERROR(f"âš ï¸ ç¬¬ {p} é æŠ“å–å¤±æ•—ï¼Œç•¥é"))
                continue

            soup = BeautifulSoup(html, "html.parser")

            # ä»¥ã€Œ12 æ¬„ä¸”ç¬¬ 1 æ¬„æ˜¯æ—¥æœŸã€ç‚ºåˆ¤æ–·ï¼Œé¿å…æŠ“åˆ°è¡¨é ­æˆ–å…¶ä»–è¡¨æ ¼
            for tr in soup.select("tr"):
                tds = [td.get_text(strip=True) for td in tr.find_all(["td", "th"])]
                if len(tds) >= 12 and DATE_RE.match(tds[0]):
                    row = dict(zip(COLUMNS, tds[:12]))

                    # å»é‡ï¼šåŒä¸€å¤©ã€åŒå¸‚å ´åªç•™ä¸€ç­†ï¼ˆä¿ç•™å…ˆé‡åˆ°çš„ï¼‰
                    key = (row["äº¤æ˜“æ—¥æœŸ"], row["å¸‚å ´"])
                    if key in seen:
                        continue
                    seen.add(key)
                    all_rows.append(row)

            time.sleep(0.5)  # ç¦®è²Œæ€§å»¶é²

        # ===== ä¾æ—¥æœŸåˆ†çµ„ï¼Œä¸¦ã€Œè£œé½Šæ‰€æœ‰å¸‚å ´ã€ =====
        by_date = {}
        for r in all_rows:
            date = r["äº¤æ˜“æ—¥æœŸ"]
            by_date.setdefault(date, {})
            by_date[date][r["å¸‚å ´"]] = {
                "å¸‚å ´": r["å¸‚å ´"],
                "é›¶å”®": {
                    "ç²³ç¨®ç™½ç±³": to_float(r["ç²³ç¨®ç™½ç±³_é›¶å”®"]),
                    "ç¡¬ç§ˆç™½ç±³": to_float(r["ç¡¬ç§ˆç™½ç±³_é›¶å”®"]),
                    "è»Ÿç§ˆç™½ç±³": to_float(r["è»Ÿç§ˆç™½ç±³_é›¶å”®"]),
                    "åœ“ç³¯ç™½ç±³": to_float(r["åœ“ç³¯ç™½ç±³_é›¶å”®"]),
                    "é•·ç³¯ç™½ç±³": to_float(r["é•·ç³¯ç™½ç±³_é›¶å”®"]),
                },
                "èº‰å”®": {
                    "ç²³ç¨®ç™½ç±³": to_float(r["ç²³ç¨®ç™½ç±³_èº‰å”®"]),
                    "ç¡¬ç§ˆç™½ç±³": to_float(r["ç¡¬ç§ˆç™½ç±³_èº‰å”®"]),
                    "è»Ÿç§ˆç™½ç±³": to_float(r["è»Ÿç§ˆç™½ç±³_èº‰å”®"]),
                    "åœ“ç³¯ç™½ç±³": to_float(r["åœ“ç³¯ç™½ç±³_èº‰å”®"]),
                    "é•·ç³¯ç™½ç±³": to_float(r["é•·ç³¯ç™½ç±³_èº‰å”®"]),
                }
            }

        # æ­£è¦åŒ–ï¼šç¢ºä¿æ¯ä¸€å¤©éƒ½æœ‰ MARKETS çš„å®Œæ•´æ¸…å–®ï¼Œç¼ºçš„å°±è£œé›¶
        normalized = {}
        for date, market_map in by_date.items():
            day_list = []
            for m in MARKETS:
                if m in market_map:
                    day_list.append(market_map[m])
                else:
                    day_list.append({
                        "å¸‚å ´": m,
                        "é›¶å”®": {k: 0.0 for k in ["ç²³ç¨®ç™½ç±³","ç¡¬ç§ˆç™½ç±³","è»Ÿç§ˆç™½ç±³","åœ“ç³¯ç™½ç±³","é•·ç³¯ç™½ç±³"]},
                        "èº‰å”®": {k: 0.0 for k in ["ç²³ç¨®ç™½ç±³","ç¡¬ç§ˆç™½ç±³","è»Ÿç§ˆç™½ç±³","åœ“ç³¯ç™½ç±³","é•·ç³¯ç™½ç±³"]},
                    })
            # ä¾å¸‚å ´åç¨±æ’åºï¼ˆå¯ä¾ä½ ç¶²ç«™é¡¯ç¤ºé †åºèª¿æ•´ï¼‰
            normalized[date] = sorted(day_list, key=lambda x: MARKETS.index(x["å¸‚å ´"]) if x["å¸‚å ´"] in MARKETS else 999)

        # ===== è¼¸å‡º JSON =====
        payload = {
            "source": URL,
            "fetched_at": timezone.now().isoformat(),
            "dates": dict(sorted(normalized.items(), reverse=True)),  # æ—¥æœŸæ–°â†’èˆŠ
        }

        out_path = Path(getattr(
            settings,
            "CRAWL_JSON_PATH",
            Path(getattr(settings, "BASE_DIR", ".")) / "data" / "crawl_result.json",
        ))
        out_path.parent.mkdir(parents=True, exist_ok=True)

        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8", dir=out_path.parent) as tf:
            json.dump(payload, tf, ensure_ascii=False, indent=2)
            tmp = tf.name
        os.replace(tmp, out_path)

        # çµ±è¨ˆ
        total_days = len(payload["dates"])
        total_rows = sum(len(v) for v in payload["dates"].values())
        self.stdout.write(self.style.SUCCESS(
            f"âœ… å®Œæˆï¼å…± {total_days} å¤©ã€{total_rows} ç­†ï¼ˆå·²è£œé½Šæ¯æ—¥ {len(MARKETS)} å¸‚å ´ï¼‰ã€‚æª”æ¡ˆï¼š{out_path}"
        ))

    # --- helpers ---

    def _fetch_page_html(self, page_num: int) -> str:
        """
        å˜—è©¦ä»¥ ?p= èˆ‡ ?page= å…©ç¨®åƒæ•¸æŠ“åˆ†é ï¼ˆéƒ¨åˆ† ASP.NET ç«™æœƒç”¨ pï¼‰ã€‚
        ä»»ä¸€æˆåŠŸå°±å›å‚³æ–‡å­—ï¼Œå¦å‰‡å› Noneã€‚
        """
        sess = requests.Session()
        for key in ("p", "page"):
            try:
                resp = sess.get(URL, headers=HEADERS, params={key: page_num}, timeout=30)
                if resp.ok and "äº¤æ˜“æ—¥æœŸ" in resp.text:
                    return resp.text
            except Exception:
                pass
        # æœ€å¾Œå†å˜—è©¦ä¸å¸¶åƒæ•¸ï¼ˆå¯èƒ½ç¬¬ä¸€é ï¼‰
        try:
            resp = sess.get(URL, headers=HEADERS, timeout=30)
            if resp.ok and "äº¤æ˜“æ—¥æœŸ" in resp.text and page_num == 1:
                return resp.text
        except Exception:
            pass
        return None
